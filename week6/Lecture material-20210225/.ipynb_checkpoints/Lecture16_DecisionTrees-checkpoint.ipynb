{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Lecture 16: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/colab_logo_32px.png)\n",
    "[Run in colab](https://colab.research.google.com/drive/1P9IoqXN9dbjJ3TN50wa8wwDdvn9P6hX7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "A class of machine learning algorithm that can perform both classification and regression\n",
    "\n",
    "- Also a fundamental component of random forests (one of the most powerful ML algorithms available) \n",
    "- We will learn how to visualise and make predictions using Decision Trees  \n",
    "\n",
    "Simple (in the first instance) conceptually as a *flow diagram* of decisions about the categories of a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Decision Tree\n",
    "\n",
    "Using the Iris Data set (see e.g. Lecture 3)\n",
    "\n",
    "Each row corresponds to an observed (*sampled*) flower, with a number of *features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "iris = load_iris() #loading data \n",
    "X = iris.data[:, 2:] # petal length and width \n",
    "y = iris.target #the answers \n",
    "\n",
    "#making a decision tree of depth 2 from the data \n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2) #sets up the function \n",
    "\n",
    "tree_clf.fit(X, y) #performs the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Lecture16_Images/iris_tree.dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3da1ba61f258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m export_graphviz(tree_clf, \n\u001b[0m\u001b[0;32m      5\u001b[0m                 \u001b[0mout_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Lecture16_Images/iris_tree.dot'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MLBDenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MLBDenv\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[1;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m             \u001b[0mout_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[0mown_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Lecture16_Images/iris_tree.dot'"
     ]
    }
   ],
   "source": [
    "#we want to visualise the actual flow diagram of the tree, for this we can use graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf, \n",
    "                out_file = 'Lecture16_Images/iris_tree.dot', \n",
    "                feature_names = iris.feature_names[ 2:], \n",
    "                class_names = iris.target_names, \n",
    "                rounded = True, \n",
    "                filled = True)\n",
    "\n",
    "#creates a dot file :( so need to convert to something more sensible\n",
    "! dot -Tpng Lecture16_Images/iris_tree.dot -o Lecture16_Images/iris_tree.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Results in \n",
    "\n",
    "<img src=\"Lecture16_Images/iris_tree.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding the output\n",
    "\n",
    "This is a 2D data with petal length and width.\n",
    "\n",
    "Data is displayed in terms of *Nodes* and *Leafs*. \n",
    "- Top node is the _root node_ \n",
    "- Lower nodes are _leaf nodes_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![iris_tree_1node.png](Lecture16_Images/iris_tree_1node.png)\n",
    "\n",
    "Arguments in the nodes are: \n",
    "- Top argument shows the _threshold_ upon which the classification division was made\n",
    "- ```gini``` (see next slides) is the quantitative measure of the _threshold_\n",
    "- ```samples```=number of training instances that satisfy the criteria \n",
    "- ```values```=number of training instances per class that satisfy the criteria \n",
    "- ```class```=prediction for the class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to Decision Trees Work?\n",
    "\n",
    "Threshold is found (using either gini or entropy measure). Then in the sub-set below threshold the process is repeated.\n",
    "\n",
    "We set ```max_depth=2```, so algorithm stopped after two divisions. \n",
    "\n",
    "<img src=\"Lecture16_Images/DT_graph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threshold Measures \n",
    "\n",
    "Two commonly used metrics: Gini and Entropy. Also called _impurity_ or _purity_ measures.\n",
    "\n",
    "### Gini \n",
    "\n",
    "$$\n",
    "G_i=1-\\sum_{k=1}^{n}p^2_{i,k}\n",
    "$$\n",
    "\n",
    "Where $p_{i,k}$ is the ratio of class k among training instances in the $i^{\\rm th}$ node. \n",
    "\n",
    "基尼指数(gini)的意义是从数据集D中随机抽取两个样本类别标识不一致的概率。基尼指数越小，数据集的纯度越高。\n",
    "\n",
    "$G_i=0$ means the sample is 100% _pure_ i.e. all instances are in a single class. $G_i=0.5$ would mean 50% are in the stated class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entropy \n",
    "\n",
    "Alternative to Gini is to use entropy as the purity measure \n",
    "$\n",
    "H_i=-\\sum_{k=1}^n p_{i,k}\\log(p_{i,k})\n",
    "$\n",
    "for $p_{i,k}\\not=0$. \n",
    "\n",
    "Does it make a difference? Not usually, although Gini tends to isolate the most frequent classes, and entropy leads to more \"balanced\" trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo the first example but use entropy instead\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2,criterion='entropy') #making a decision tree of depth 2 from the data \n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "export_graphviz(tree_clf, \n",
    "                out_file = 'Lecture16_Images/iris_tree_entropy.dot', \n",
    "                feature_names = iris.feature_names[ 2:], \n",
    "                class_names = iris.target_names, \n",
    "                rounded = True, \n",
    "                filled = True)\n",
    "! dot -Tpng Lecture16_Images/iris_tree_entropy.dot -o Lecture16_Images/iris_tree_entropy.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Entropy\n",
    "<!-- <img src=\"Lecture16_Images/iris_tree_entropy.png\" alt=\"Drawing\" style=\"width: 250px;\"/> -->\n",
    "![](Lecture16_Images/iris_tree_entropy.png)\n",
    "\n",
    "Gini\n",
    "<!-- <img src=\"Lecture16_Images/iris_tree.png\" alt=\"Drawing\" style=\"width: 250px;\"/> -->\n",
    "![](Lecture16_Images/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification And Regression Tree (CART) Algorithm\n",
    "\n",
    "SciKit Learn uses the CART algorithm to do the thresholding. \n",
    "\n",
    "- Splits the sample into two subsets using a single feature $k$ at threshold $t_k$\n",
    "- Chooses $k$ and $t_k$ by finding pair that produces purest subset\n",
    "\n",
    "Cost function of the splits is \n",
    "$\n",
    "J(k,t_k)=\\frac{m_{\\rm left}}{m}G_{\\rm left}+\\frac{m_{\\rm right}}{m}G_{\\rm right}\n",
    "$\n",
    "i.e. the number-weighted purity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that CART \n",
    "\n",
    "- Only splits in two i.e. is binary \n",
    "- Is a greedy algorithm i.e. only considers level n optimisation not n+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Probabilities \n",
    "\n",
    "Also want to know the _probability_ that an instance $i$ belongs to class $k$. \n",
    "\n",
    "Does this by finding the leaf node for instance $i$, then returns ratio of training instances of class $k$ in this node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example \n",
    "For case where flower has petals=5cm long and 1.5cm wide\n",
    "corresponding leaf node is at depth-2 left node. \n",
    "\n",
    "<img src=\"Lecture16_Images/DT_graph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![iris_tree_1node2.png](Lecture16_Images/iris_tree_1node2.png)\n",
    "\n",
    "So probabilities are: 0% (Setosa), 49/54=90.7% (Versicolor), 5/54=9.3% (Virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() \n",
    "X = iris.data[:, 2:] # petal length and width \n",
    "y = iris.target #classifications \n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2) #making a decision tree of depth 2 from the data \n",
    "tree_clf.fit(X, y) #do the fits\n",
    "\n",
    "## for case where flower has petals=5cm long and 1.5cm wide\n",
    "## corresponding leaf node is at depth-2 left node\n",
    "# coordinates below are X coordinates \n",
    "tree_clf.predict_proba([[5, 1.5]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Optimisation \n",
    "\n",
    "Decision Trees are **non-parametric** classification algorithms. \n",
    "\n",
    "Tends to overfit if not careful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Need to regularise the problem. In Decision Trees this can be done by \n",
    "- Restricting the maximum depth of the tree ```max_depth``` in SciKit-Learn\n",
    "- ```min_samples_split``` is mimimum number of samples before a node can be split \n",
    "- ```min_samples_leaf``` the minimum number a leaf can have (or ```min_weight_fraction``` expressed as a fraction of total samples)\n",
    "- ```max_leaf_nodes``` maximum number of leaf nodes \n",
    "- ```max_features``` maximum number of features used in splitting a node\n",
    "\n",
    "Other packages _prune_ i.e. make a (relatively) unrestricted tree then remove statistically insignificant nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Use the make_moons data set SciKit Learn to make a Decision Trees based on: \n",
    "- 1) The default parameters \n",
    "- 2) When ```min_samples_leaf=4```\n",
    "- 3) Plot the result and thresholds in a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two DTs on moons dataset.\n",
    "# left: default params = no restrictions (case of overfitting)\n",
    "# right: min_samples_leaf = 4. (better generalization)\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#create the data set X (features) and Y(classifications)\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53) \n",
    "\n",
    "#make two DTs one with min_samples_leaf\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "#save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees as Regression \n",
    "\n",
    "Can also use Decision Trees to do regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor #uses a different module from SciKit Learn\n",
    "\n",
    "tree_reg=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y) #uses Iris data \n",
    "\n",
    "#this time trying to fin relation between X and y \n",
    "\n",
    "export_graphviz(tree_reg, \n",
    "                out_file = 'Lecture16_Images/iris_tree_reg.dot', \n",
    "                feature_names = iris.feature_names[ 2:], \n",
    "                class_names = iris.target_names, \n",
    "                rounded = True, \n",
    "                filled = True)\n",
    "! dot -Tpng Lecture16_Images/iris_tree_reg.dot -o Lecture16_Images/iris_tree_reg.png\n",
    "#tree in this case doesn't really help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Lecture16_Images/iris_tree_reg.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "CART algorithm works in the same way except the cost function of the splits is \n",
    "$\n",
    "J(k,t_k)=\\frac{m_{\\rm left}}{m}{\\rm MSE}_{\\rm left}+\\frac{m_{\\rm right}}{m}{\\rm MSE}_{\\rm right}\n",
    "$\n",
    "i.e. the number-weighted MSE measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a more intuitive example \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy.random as rnd\n",
    "\n",
    "# Quadrat!ic training set + noise\n",
    "rnd.seed(42)\n",
    "m = 200\n",
    "X = rnd.rand(m, 1) #random x\n",
    "y = 4 * (X - 0.5) ** 2 #quadratic function\n",
    "y = y + rnd.randn(m, 1) / 10 #random in y\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting stuff\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4)), plt.subplot(121), plot_regression_predictions(tree_reg1, X, y)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15), plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13), plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122), plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13), plt.title(\"max_depth=3\", fontsize=14)\n",
    "# Predicted value for each region (red line) = avg target value of instances in that region.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Issues \n",
    "\n",
    "Decision Trees tend to make Orthogonal Boundaries (splits perpendicular to an axis). So sensitive to: \n",
    "- rotation of axis or transforms of variables\n",
    "- training set selection. e.g. if one removes just one point from the Iris example get\n",
    "\n",
    "<img src=\"Lecture16_Images/unstable.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "*Random Forests* get over some of these issues by averaging over many trees (see Lecture 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise_pointer"
    ]
   },
   "source": [
    "### You can now do Exercises 1-2 in Lecture16_DecisionTrees_Exercises.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
